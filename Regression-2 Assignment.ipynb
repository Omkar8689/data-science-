{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a088a13-f9f7-45bd-bc12-bca8e4ad2ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccd049b-cb6c-440a-b83d-f25b4a363590",
   "metadata": {},
   "source": [
    "R-squared (coefficient of determination) is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a regression model. In other words, it assesses the goodness of fit of the model, indicating the percentage of variability in the response variable that can be accounted for by the predictor variables.\n",
    "\n",
    "R-squared values range from 0 to 1. A higher R-squared value indicates a better fit of the model to the data because a larger proportion of the variability in the dependent variable is explained by the independent variables. However, R-squared alone does not provide information about the model's appropriateness or the significance of individual predictors. It is often used in conjunction with other statistical measures and should be interpreted in the context of the specific study.\n",
    "\n",
    "You can easily calculate it using the sklearn libraries like sklearn.metrics.r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c8d789-cae6-4795-a07c-cb1eeebf92e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d69ce7-9a56-4d9c-982d-b3cbc82a1fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0208c20e-178b-40df-ace1-be51954aa9a0",
   "metadata": {},
   "source": [
    "\n",
    "Here's an explanation of adjusted R-squared and how it differs from R-squared:\n",
    "\n",
    "R-squared (R²):\n",
    "It's a statistical measure that indicates how well the regression model fits the observed data.\n",
    "It ranges from 0 to 1, where:\n",
    "0 indicates that the model doesn't explain any of the variation in the dependent variable.\n",
    "1 indicates that the model perfectly explains all of the variation in the dependent variable.\n",
    "It's calculated as the proportion of the total variation in the dependent variable that's explained by the independent variables.\n",
    "\n",
    "Adjusted R-squared (R²_adj):\n",
    "It's a modified version of R-squared that penalizes the addition of irrelevant variables to the model.\n",
    "It accounts for the number of predictors (independent variables) in the model, making it a more reliable measure of fit, especially when comparing models with different numbers of predictors.\n",
    "It's always less than or equal to R-squared.\n",
    "It can even be negative, indicating that the model is a poor fit and that the independent variables explain less of the variation than would be expected by chance.\n",
    "\n",
    "Key differences:\n",
    "R-squared always increases (or stays the same) when you add more predictors, even if those predictors are irrelevant.\n",
    "Adjusted R-squared increases only when the added predictors genuinely improve the model's fit. It can decrease if irrelevant predictors are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22462c4b-074f-478f-b977-a5ea6e4fcef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a974a27-e067-4ff2-8516-663a9d361a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7602ac75-0edf-48a1-9d66-7dfd223d372f",
   "metadata": {},
   "source": [
    "\n",
    "Here are the situations where it's more appropriate to use adjusted R-squared:\n",
    "\n",
    "Comparing Models with Different Numbers of Predictors:\n",
    "When you have multiple regression models with varying numbers of predictors, adjusted R-squared is the preferred metric for comparison. It accounts for the added complexity of more predictors, ensuring a fair assessment of how well each model fits the data, independent of its size.\n",
    "\n",
    "Assessing Model Parsimony:\n",
    "Adjusted R-squared penalizes models for including irrelevant predictors. It can help you identify the most parsimonious model (the one with the fewest predictors that still adequately explains the data). This is crucial because simpler models are often more interpretable, generalizable, and less prone to overfitting.\n",
    "\n",
    "Evaluating Potential Overfitting:\n",
    "Overfitting occurs when a model becomes too complex and captures noise in the data rather than true patterns. Adjusted R-squared can signal potential overfitting by decreasing when added predictors don't genuinely improve the model's fit.\n",
    "\n",
    "Selecting Variables for Prediction:\n",
    "When building a predictive model, adjusted R-squared can guide variable selection. It helps you choose the predictors that contribute most to the model's accuracy, leading to a more efficient and focused model.\n",
    "Dealing with Small Sample Sizes:\n",
    "\n",
    "Adjusted R-squared is more reliable than R-squared when working with small samples. It helps prevent artificially high R-squared values that can occur due to chance correlations in small datasets.\n",
    "\n",
    "In general, use adjusted R-squared when:\n",
    "You're comparing multiple models with different numbers of predictors.\n",
    "You prioritize model parsimony and want to avoid overfitting.\n",
    "You're working with small sample sizes.\n",
    "You're focused on prediction accuracy and need to select the most relevant predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0837e42-4153-4920-b85a-fe7429604509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b330d849-ccab-40a1-9101-932aefef456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a275e-97b1-40f1-b6e5-2b4f82eb092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE=Mean Squared Error\n",
    "It measures the average squared difference between the actual values (y_true)\n",
    "and the predicted values (y_pred) in a regression model.\n",
    "Calculation:\n",
    "MSE = 1/n * Σ(y_true - y_pred)^2\n",
    "\n",
    "MAE=Mean Absolute Error\n",
    "measures the average absolute difference between the actual and predicted values.\n",
    " Calclulation:\n",
    "        MAE=1/n *  Σ(y_true - y_pred)\n",
    "\n",
    "RMSE=It is the square root of MSE.Its used for the getting small difference between the actual\n",
    "value and predicted value.\n",
    "Calculation:\n",
    "    RMSE=sqrt(MSE)\n",
    "\n",
    "    \n",
    "    You can also use the sklearn library to calculate this\n",
    "    sklearn.metrics.mean_absolute_error -This library is used to calculate the MAE.\n",
    "    sklearn.metrics.mean_squared_error -This library is used to calculate the MSE\n",
    "    Also you can use the numpy to calculate the square root of the MSE\n",
    "    RMSE=np.sqrt(MSE)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a1d3f9-5042-43dd-9b84-1809b362049f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e66b5f6-f01b-4272-be1f-628bfe05c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb7e02-afdc-464c-ad87-6e2c2e54650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Comparing Models with Different Numbers of Predictors:\n",
    "\n",
    "When you have multiple regression models with varying numbers of predictors, adjusted R-squared is the preferred metric for comparison. It accounts for the added complexity of more predictors, ensuring a fair assessment of how well each model fits the data, independent of its size.\n",
    "Assessing Model Parsimony:\n",
    "\n",
    "Adjusted R-squared penalizes models for including irrelevant predictors. It can help you identify the most parsimonious model (the one with the fewest predictors that still adequately explains the data). This is crucial because simpler models are often more interpretable, generalizable, and less prone to overfitting.\n",
    "Evaluating Potential Overfitting:\n",
    "\n",
    "Overfitting occurs when a model becomes too complex and captures noise in the data rather than true patterns. Adjusted R-squared can signal potential overfitting by decreasing when added predictors don't genuinely improve the model's fit.\n",
    "Selecting Variables for Prediction:\n",
    "\n",
    "When building a predictive model, adjusted R-squared can guide variable selection. It helps you choose the predictors that contribute most to the model's accuracy, leading to a more efficient and focused model.\n",
    "Dealing with Small Sample Sizes:\n",
    "\n",
    "Adjusted R-squared is more reliable than R-squared when working with small samples. It helps prevent artificially high R-squared values that can occur due to chance correlations in small datasets.\n",
    "In general, use adjusted R-squared when:\n",
    "\n",
    "You're comparing multiple models with different numbers of predictors.\n",
    "You prioritize model parsimony and want to avoid overfitting.\n",
    "You're working with small sample sizes.\n",
    "You're focused on prediction accuracy and need to select the most relevant predictors.\n",
    "profile picture\n",
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "Here's an explanation of RMSE, MSE, and MAE in regression analysis:\n",
    "\n",
    "1. Mean Squared Error (MSE):\n",
    "\n",
    "It measures the average squared difference between the actual values (y_true) and the predicted values (y_pred) in a regression model.\n",
    "Calculation:\n",
    "MSE = 1/n * Σ(y_true - y_pred)^2\n",
    "Interpretation:\n",
    "Lower MSE indicates a better fit.\n",
    "MSE is sensitive to large errors due to squaring.\n",
    "2. Root Mean Squared Error (RMSE):\n",
    "\n",
    "It's the square root of MSE.\n",
    "Calculation:\n",
    "RMSE = √(MSE)\n",
    "Interpretation:\n",
    "It has the same units as the target variable, making it easier to interpret.\n",
    "Lower RMSE indicates a better fit.\n",
    "Similar to MSE, it's sensitive to large errors.\n",
    "3. Mean Absolute Error (MAE):\n",
    "\n",
    "It measures the average absolute difference between the actual and predicted values.\n",
    "Calculation:\n",
    "MAE = 1/n * Σ|y_true - y_pred|\n",
    "Interpretation:\n",
    "Lower MAE indicates a better fit.\n",
    "It's less sensitive to large errors than MSE or RMSE.\n",
    "Key Points:\n",
    "\n",
    "All three metrics measure prediction error but have different sensitivities to error size.\n",
    "MSE and RMSE are more sensitive to large errors than MAE.\n",
    "RMSE is often preferred for its interpretability and direct comparability with the target variable's scale.\n",
    "MAE is useful when you want to measure error magnitude without excessive influence from outliers.\n",
    "Choosing the right metric depends on:\n",
    "\n",
    "The distribution of errors in your data.\n",
    "Your sensitivity to large errors.\n",
    "The importance of interpretability.\n",
    "profile picture\n",
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "View other drafts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Advantages and Disadvantages of RMSE, MSE, and MAE\n",
    "Choosing the right metric to evaluate your regression model is crucial for understanding its performance and limitations. Here's a breakdown of the advantages and disadvantages of RMSE, MSE, and MAE:\n",
    "\n",
    "1. Mean Squared Error (MSE):\n",
    "Advantages:\n",
    "Easy to interpret: MSE reflects the average squared difference between predicted and actual values. Higher squared differences point to larger errors.\n",
    "Differentiable: MSE is differentiable, making it ideal for optimization algorithms used in model training.\n",
    "Commonly used: MSE is a widely recognized metric, allowing for easier comparison with existing models and benchmarks.\n",
    "\n",
    "Disadvantages:\n",
    "Sensitive to outliers: Large errors are heavily penalized due to squaring, potentially skewing the overall evaluation.\n",
    "Loses unit interpretation: MSE results are in squared units of the dependent variable, making it less intuitive to understand.\n",
    "Focuses on average error: Doesn't consider the distribution of errors, potentially masking small but consistent discrepancies.\n",
    "\n",
    "2. Root Mean Squared Error (rmse):\n",
    "Advantages:\n",
    "Shares advantages of MSE: Easy to interpret, differentiable, and commonly used.\n",
    "Shares units with target variable: RMSE units match those of the dependent variable, facilitating interpretation of error magnitude.\n",
    "\n",
    "Disadvantages:\n",
    "Inherits disadvantages of MSE: Still sensitive to outliers and focuses on average error, neglecting error distribution.\n",
    "Provides less information than MSE: Losing the squared differences removes detailed information about error severity.\n",
    "\n",
    "3. Mean Absolute Error (MAE):\n",
    "Advantages:\n",
    "Robust to outliers: MAE uses absolute differences, making it less affected by extreme error values.\n",
    "Focuses on average magnitude: Captures the average size of errors, giving a better sense of error distribution.\n",
    "Easy to understand: Absolute differences are easier to interpret than squared errors.\n",
    "\n",
    "Disadvantages:\n",
    "Not differentiable: MAE is not differentiable, making it unsuitable for some optimization algorithms.\n",
    "Doesn't penalize large errors: Unlike MSE and RMSE, MAE treats all errors equally, potentially masking significant discrepancies.\n",
    "Less common: MAE is less frequently used compared to MSE and RMSE, limiting direct comparison with established benchmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664784ab-e94f-4801-9faf-d1a569362b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13616ed-f1a8-414e-9ed0-c4a103023485",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab3a40-c6dc-4451-a9b5-b09cffb67e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso Regularization: Shrinking Coefficients and Sparsity\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) is a regularization technique used in linear regression models to prevent overfitting and improve model interpretability. Here's how it works:\n",
    "\n",
    "Idea: Lasso adds a penalty term to the cost function during model training. This penalty penalizes large absolute values of the regression coefficients, effectively shrinking them towards zero. By shrinking coefficients, Lasso:\n",
    "\n",
    "Reduces model complexity: Fewer non-zero coefficients lead to a simpler model less prone to overfitting.\n",
    "Improves interpretability: Identifying which coefficients become zero helps understand the features with the most significant impact on the model.\n",
    "Performs feature selection: If a coefficient becomes exactly zero, the corresponding feature is effectively removed from the model.\n",
    "Comparison with Ridge Regularization:\n",
    "\n",
    "Both Lasso and Ridge regularize models by penalizing coefficients, but they differ in the type of penalty:\n",
    "\n",
    "Lasso: Uses L1 regularization, penalizing the absolute sum of all coefficients.\n",
    "Ridge: Uses L2 regularization, penalizing the sum of squared coefficients.\n",
    "This difference leads to key distinctions:\n",
    "\n",
    "Feature selection: Lasso can set coefficients to zero, achieving automatic feature selection and identifying the most relevant features. Ridge only shrinks coefficients, never eliminating them.\n",
    "Sparsity: Lasso models tend to be sparser, with fewer non-zero coefficients, leading to simpler and potentially more interpretable models. Ridge models generally have more non-zero coefficients.\n",
    "Sensitivity to outliers: Lasso is more sensitive to outliers because the absolute penalty can be heavily influenced by extreme values. Ridge is less affected.\n",
    "When to use Lasso:\n",
    "\n",
    "Feature selection is crucial: When understanding the most influential features is important, Lasso's ability to set coefficients to zero is valuable.\n",
    "Model interpretability is a priority: Sparser models with fewer non-zero coefficients are easier to interpret and understand.\n",
    "Overfitting is a concern: In complex datasets with high dimensionality, Lasso's ability to reduce model complexity helps prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397d33df-1cdf-4679-a0a0-45b08b40babb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ef5d4-f708-44a3-be49-fde6d6cd7bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3365be9c-00db-4894-8231-df0b73a1c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Here's how regularized linear models help prevent overfitting in machine learning, along with an illustrative example:\n",
    "\n",
    "Overfitting:\n",
    "\n",
    "It occurs when a model learns the training data too closely, including its noise and random fluctuations.\n",
    "This leads to poor performance on new, unseen data, as the model focuses on irrelevant patterns instead of true relationships.\n",
    "Regularization to the Rescue:\n",
    "\n",
    "Regularization techniques aim to reduce overfitting by adding constraints to the learning process.\n",
    "In linear models, they achieve this by penalizing large model coefficients, effectively simplifying the model and making it less prone to capturing noise.\n",
    "Key Mechanisms:\n",
    "\n",
    "Shrinking Coefficients: Regularization shrinks the values of the coefficients (weights) associated with the features in the model. This prevents any single feature from having an overly dominant influence, reducing the model's sensitivity to noise and outliers.\n",
    "\n",
    "Encouraging Simpler Models: Regularization discourages complex models with many non-zero coefficients. This preference for simpler models often leads to better generalization to new data, as they capture broader patterns rather than specific details in the training data.\n",
    "\n",
    "Example: Predicting Housing Prices:\n",
    "\n",
    "Problem: You're building a linear model to predict housing prices based on features like house size, number of rooms, neighborhood, proximity to schools, etc.\n",
    "Overfitting Risk: Using all features without regularization might lead to a model that captures noise and irrelevant patterns in the training data, such as a specific house's unique features or unusual pricing in a particular neighborhood.\n",
    "Regularization Solution: By applying L1 (Lasso) or L2 (Ridge) regularization, you can:\n",
    "Shrink coefficients of less relevant features, reducing their impact on predictions.\n",
    "Potentially eliminate some features entirely (Lasso), focusing on the most important predictors.\n",
    "Create a simpler model that captures broader trends in housing prices, generalizing better to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1affce55-f9bc-42e5-ab82-6d2344f6374f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f184e70-565c-460d-9d4b-ec6918c16929",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52456eaa-588e-4d55-9449-de474a4788d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linearity Assumption:\n",
    "\n",
    "Regularized linear models assume a linear relationship between the features and the target variable.\n",
    "If this assumption doesn't hold, model performance can suffer significantly.\n",
    "Non-linear relationships, complex interactions, or highly non-linear patterns require more flexible models like decision trees, support vector machines, or neural networks.\n",
    "\n",
    "2. Sensitivity to Outliers (Lasso):\n",
    "Lasso regularization, with its L1 penalty, is particularly sensitive to outliers in the data.\n",
    "Outliers can disproportionately influence coefficient values, leading to suboptimal models.\n",
    "If outliers are a concern, Ridge regularization or robust regression techniques are often better choices.\n",
    "\n",
    "3. Potential Bias Introduction:\n",
    "Regularization inherently introduces bias into model estimates by shrinking coefficients towards zero.\n",
    "While this can reduce variance and improve generalization, it can also lead to underfitting if model complexity is overly restricted.\n",
    "Finding the right balance between bias and variance through hyperparameter tuning is crucial.\n",
    "\n",
    "4. Limitations in High-Dimensional Settings:\n",
    "In datasets with a vast number of features, regularization might not be enough to fully combat overfitting.\n",
    "Dimensionality reduction techniques, careful feature selection, or non-linear models might be necessary for effective analysis.\n",
    "\n",
    "5. Interpretability Trade-offs:\n",
    "While regularization can improve interpretability by simplifying models, it can also make interpretation more challenging in certain cases.\n",
    "For example, Ridge regularization keeps all features in the model, even with small coefficients, making feature importance identification less clear.\n",
    "\n",
    "6. Computational Cost (Lasso):\n",
    "Solving for Lasso's L1 penalty can be computationally more expensive than Ridge's L2 penalty, especially with large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da7415c-2b51-4ad9-99b8-c4a4876bcf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d44746-4df2-4866-a216-4335f5c30666",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f8656b-ac64-4532-a401-440ba9d561a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "I will choose the RMSR if there are the outliers presert in the dataset.\n",
    "And if the outliers are not more important then and then only I will choose the MAE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67282bd8-70fa-4105-aac6-273589abfed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abd2bf6-6319-47b4-8878-276f019ab592",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a2fd5b-2fe9-4f03-b072-be5b7c5f6e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963ee8a-7bd0-4f95-b817-fd1700637df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
