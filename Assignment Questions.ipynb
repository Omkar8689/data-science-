{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7553cb29-3ff4-4714-9b58-78da121101cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8cdc78-2c0c-4897-b9a4-61a0a48e5a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce604a9-c5d8-4609-8a9c-ae4b1fa1b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN stands for the K nearest neighbour algorithm that is used to solve  both the classification as well as regressor problems.\n",
    "This algorithm tries to find the distance between the data points and returns the point that is very close to that point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350fc54-e065-4ca6-ad8c-8bc52adfaa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57cc98d-b4b2-4b58-acd0-ac101c4f22eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "You can choose the value of k manually . The value of the K that gives the highest accuracy we will take it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bf0f0c-5b73-4624-b72d-87c50eca3a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142ff30e-011d-4b55-a618-ef3e35ea6217",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99e82c-8058-4f14-adae-fd043a66d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "The main differance between the KNN classifier and KNN regressor is that KNN classifier is used to\n",
    "multiple classification and KNN Regressor is used for the Binary Classification.\n",
    "Example -\n",
    "1) KNN classifier:\tEmail spam classification\t\n",
    "2) KNN regressor:    House price prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb58976-1090-4989-a348-d3aab0a04952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aff941-71be-4f8b-ba0d-f5ad1759c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b577d-0b14-4916-876b-715acc3f4d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "To measure the performance of the KNN classifier you may use the accuracy matrix,classification_report,confusion_matrix.\n",
    "To measure the performance of the KNN Regressor you may use the MSE,MAE that is Mean Squared Error,Mean Accuracy Error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d380e-bedc-4dec-8878-6a4f9dea48b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463a84a5-5994-4878-8406-e9162b424697",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a442c8be-1333-47f7-826c-0ee59cb97db4",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm suffers from the curse of dimensionality. This refers to the challenges that arise in machine learning when dealing with high-dimensional data (data with many features). Here's how it impacts KNN:\n",
    "\n",
    "Increased Sparsity: As the number of dimensions increases, the data becomes sparser in the feature space. Imagine data points scattered like stars in the sky. In high dimensions, these points get spread out very far apart, making it difficult to find truly \"nearest neighbors\" for a new data point.\n",
    "\n",
    "Irrelevant Features: With many features, some might be irrelevant or redundant for the task at hand. These irrelevant features can mislead the distance calculation between data points, causing the KNN model to consider distant points as neighbors and neglecting closer relevant ones.\n",
    "\n",
    "Distance Metric Issues: In high dimensions, the distance metric used by KNN (like Euclidean distance) might become less meaningful. The distances between points can become more similar, making it harder to distinguish between truly close and faraway neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03192886-f7af-49a5-a2b6-ee07c55f0fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431b0d3-4194-43d5-97e9-961a0103c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf58650f-ef50-4b45-9a1b-89940f20990d",
   "metadata": {},
   "source": [
    "KNN imputation is a common technique to handle missing values in KNN algorithms. It leverages the k-Nearest Neighbors principle to estimate the missing values based on similar data points. Here's how it works:\n",
    "\n",
    "Identify Missing Values:  First, you need to identify and mark missing values in your data. Libraries like scikit-learn use NaN (Not a Number) to represent missing entries.\n",
    "\n",
    "Find Nearest Neighbors:  For each data point with missing values, the algorithm identifies the k closest neighbors in the training set based on the available features (attributes without missing values). You'll need to choose an appropriate distance metric (e.g., Euclidean distance) to determine closeness.\n",
    "\n",
    "Impute Missing Values:  Once the k nearest neighbors are identified, KNN imputation estimates the missing value(s) for the data point. This estimation can be done in a couple of ways:\n",
    "\n",
    "Mean Imputation: The most common approach is to calculate the average (mean) of the corresponding feature values from the k nearest neighbors. This imputed value replaces the missing value in the data point.\n",
    "Weighted Mean Imputation: A refinement of mean imputation, where weights are assigned to the neighbors based on their distance to the data point. Closer neighbors contribute more weight to the average, potentially leading to a more accurate imputation.\n",
    "Mode Imputation (for categorical data): If you're dealing with categorical data with missing values, KNN imputation can find the most frequent category (mode) among the k nearest neighbors and use that as the imputed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587f8b3e-b4ea-416d-b462-5dc776f45805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90edb755-f274-4b7b-98f5-31b27f1e34a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24202e2c-c314-4df8-b543-d13f7e43389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Target Variable: KNN classifiers deal with discrete target variables (categories), while KNN regressors handle continuous numerical target variables.\n",
    "Prediction Method: Classifiers predict the most frequent class label among the k nearest neighbors. Regressors predict the average (or median/weighted average) of the target variable values from the k nearest neighbors.\n",
    "Sensitivity to Noise: KNN classifiers can be more sensitive to noise in the data compared to KNN regressors. Outliers or noisy data points might significantly influence the class label prediction in a classifier, while their impact might be averaged out in a regression prediction.\n",
    "Which one is better for which type of problem?\n",
    "\n",
    "\n",
    "KNN Classifier (better for):\n",
    "Classification problems with well-defined categories (e.g., spam detection, image classification).\n",
    "Smaller datasets where interpretability is important.\n",
    "\n",
    "KNN Regressor (better for):\n",
    "Regression problems where you want to predict a continuous value (e.g., house price prediction, customer churn prediction).\n",
    "Data with less noise, as KNN regressors might be more robust to outliers.\n",
    "\n",
    "General Considerations:\n",
    "Both KNN classifiers and regressors can be effective for specific tasks, but they might not be the best choice for all problems, especially with high-dimensional data.\n",
    "Other algorithms, such as Support Vector Machines (SVMs) for classification or linear regression for continuous predictions, might outperform KNN in certain scenarios.\n",
    "It's always recommended to compare different algorithms on your specific dataset to determine the best performer for your needs.\n",
    "In conclusion, the choice between a KNN classifier and regressor depends on the problem type (classification vs. regression) and the nature of your data. Consider factors like interpretability needs, noise sensitivity, and data dimensionality when making your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f56ac1-5f3c-4917-9126-492d548f6fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4ba98-2e21-4494-9ccb-ea82ac2c4b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09d794-304f-4809-8be6-6cc6efff8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "Strengths:\n",
    "Simplicity and Interpretability: KNN is a conceptually easy-to-understand algorithm. It doesn't involve complex model building, making it interpretable. You can identify the nearest neighbors that influenced the prediction for a new data point.\n",
    "Versatility: KNN can be used for both classification (predicting categories) and regression (predicting continuous values) tasks.\n",
    "No assumptions about data: Unlike some algorithms that require specific data distributions, KNN works with various data types without strict assumptions.\n",
    "Effective for certain datasets: KNN can perform well for smaller datasets with well-defined clusters, especially when interpretability is important.\n",
    "\n",
    "Weaknesses:\n",
    "Curse of Dimensionality: In high-dimensional data (many features), finding meaningful nearest neighbors becomes difficult, leading to decreased accuracy and overfitting.\n",
    "Computationally expensive: Classifying or predicting new data points requires comparing them to all data points in the training set, making it slow for large datasets.\n",
    "Sensitive to noise: Outliers or noisy data points can significantly influence KNN predictions, especially for classification tasks.\n",
    "Choice of K and distance metric: The optimal value of K (number of neighbors) and the distance metric used can significantly impact KNN performance.\n",
    "\n",
    "Addressing the Weaknesses:\n",
    "Dimensionality Reduction: Techniques like Principal Component Analysis (PCA) can reduce features while preserving important information, mitigating the curse of dimensionality.\n",
    "Data Preprocessing: Standardizing or normalizing data ensures features are on a similar scale, reducing the influence of irrelevant features on distance calculations.\n",
    "KD-Trees: For large datasets, using data structures like KD-Trees can improve efficiency by speeding up nearest neighbor searches.\n",
    "K Selection Techniques: Techniques like cross-validation or the elbow method can help find the optimal value of K.\n",
    "Alternative Distance Metrics: Explore distance metrics like Manhattan distance or cosine similarity that might be more suitable for your data than Euclidean distance.\n",
    "Consider alternative algorithms: Depending on the problem and data characteristics, algorithms like Support Vector Machines (SVMs) for classification or linear regression for continuous predictions might outperform KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2becc24-85dd-4cbf-b925-438293367b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c90429-5b16-4df0-bd37-ac08c68fd2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d36c3-e97a-465b-b923-dd201dfd4a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature\t-:   Euclidean Distance                    \tManhattan Distance\n",
    "Calculation\tsqrt(sum of squared differences)\tSum of absolute differences\n",
    "Visualization\tStraight line\tGrid-like path\n",
    "Focus\tOverall magnitude\tTotal sum of differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cceb228-9ede-49f1-9090-ae4425f3151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0ba31-b1ef-4f9a-b2d9-0d292a949f26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
