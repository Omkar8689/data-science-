{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea7c1e0-3053-47b8-9b58-bf8e96b47ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfcf3dd-8096-47e4-9350-9123d8dcf94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso Regression is the type of Regression which is used for the feature selection process.\n",
    "In this technique we use the just slope rather than square of the slope which makes it \n",
    "differ from other regression technique.The Lasso Regression is calculated as following formula-\n",
    "  \n",
    "    lasso regression= 1/n * sum(y1-y2)^2  + k(sum |slope|)\n",
    "    where y1=actual value\n",
    "          y2=predicted value\n",
    "          k=hyper parameter.\n",
    "            \n",
    "            \n",
    "            \n",
    "            Also you can use the 'sklearn.linear_model.Lasso' library to calculate the lasso regression\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9bc4a2-5031-48f0-8c69-4ba78c9ee101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2d792-4f07-44df-afe1-0b1c8387c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b00048-0e1f-4823-a987-dce662a55e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "The main advantage of using Lasso Regression in feature selection \n",
    "is it can give the best features for model making and training, testing purposes.\n",
    "\n",
    "Also it Prevents Overfitting:  Lasso Regression helps prevent overfitting by penalizing the model for\n",
    "using too many features. This is particularly useful when dealing with a high-dimensional dataset \n",
    "with many features, as it allows the model to generalize better to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb7c193-fee5-482f-8551-e9fd80d2aa03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9ebb1-90e8-466b-bc1a-c30d8c1d7d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b278a95a-788c-4b8e-b248-577b8c18ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Interpreting the coefficients of a Lasso Regression model involves understanding the \n",
    "impact of each feature on the target variable, considering the effects of regularization. \n",
    "In Lasso Regression, the regularization term (L1 penalty) encourages sparsity in the model,\n",
    "leading some coefficients to be exactly zero. \n",
    "  Here's how you can interpret the coefficients:\n",
    "\n",
    "Non-Zero Coefficients:\n",
    "If the coefficient of a feature is non-zero, it means that the corresponding feature has a\n",
    "non-negligible impact on the prediction. The sign (positive or negative) indicates the direction of the impact,\n",
    "and the magnitude indicates the strength.\n",
    "\n",
    "Zero Coefficients:\n",
    "If the coefficient of a feature is exactly zero, it suggests that the Lasso Regression has\n",
    "effectively eliminated that feature from the model. The model considers that feature to be less important\n",
    "for predicting the target variable.\n",
    "\n",
    "Variable Selection:\n",
    "Lasso Regression, with its sparsity-inducing property, performs automatic feature selection.\n",
    "The selected features (those with non-zero coefficients) are considered important contributors to the model,\n",
    "while the eliminated features (those with zero coefficients) are deemed less relevant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744b2a9-96ed-4f5d-97de-3199855f5a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8369e8d3-53a7-4402-a420-0670b180b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447284f2-0729-415a-889a-82c46da4dc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "The tuning parameter plays a vital role in the models performance as it used for the feature selection.\n",
    "It gives the best features that are corelated with each other and removes the features that \n",
    "are not more corerelated and also not more important. As you will increase the value of the tuning parameter \n",
    "it will decrease the value of the theta or coefficient and reach to 0 and vice versa.  \n",
    "   As the  feature selection is the main aspect of any ml project it must be done in correct way.\n",
    "    If it is not done in proper way it may affect the models accuracy and efficiency.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c33e023-9984-4859-b2c6-91af427e88a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5fd86b-140a-4833-ad59-beb580827ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7bf624-fe8f-4893-9b8b-9eefe5a0b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso Regression is a linear regression technique that incorporates regularization by adding a penalty term based on the absolute values of the coefficients. It is primarily designed for linear models and may not perform well for non-linear regression problems on its own.\n",
    "\n",
    "However, there are ways to adapt Lasso Regression for non-linear regression problems by incorporating non-linear features or transformations. Here are a few approaches:\n",
    "\n",
    "Feature Engineering:\n",
    "\n",
    "Introduce non-linear features: Create new features by applying non-linear transformations to the existing features. For example, you can include squared or cubed terms of the original features.\n",
    "Interaction terms: Include interaction terms between different features.\n",
    "Polynomial Regression:\n",
    "\n",
    "Transform your data using polynomial features. You can use Polynomial Regression, which is an extension of linear regression but includes polynomial terms of the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dcff03-1dc5-465a-89f4-0c50d82c832b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44560bb-7044-47c5-8c80-ddddd9e26024",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695bf355-bbd9-4d4b-b2be-84c24d29b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "There is the following difference found in the Ridge Regression and Lasso Regression.\n",
    "\n",
    "Ridge Regression is also known as L2 regression\n",
    "Lasso Regression is also known as L1 regression.\n",
    "\n",
    "Ridge Regression is used using  the square of the slope\n",
    "lasso regression is used using the just the slope.\n",
    "\n",
    "\n",
    "  Ridge regression= 1/n * sum(y1-y2)^2  + k(sum |slope|)^2\n",
    "    where y1=actual value\n",
    "          y2=predicted value\n",
    "          k=hyper parameter.\n",
    "\n",
    "    lasso regression= 1/n * sum(y1-y2)^2  + k(sum |slope|)\n",
    "    where y1=actual value\n",
    "          y2=predicted value\n",
    "          k=hyper parameter.\n",
    "            \n",
    "            \n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be5017-65c0-48aa-b490-3fa5efd1382d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e549929-f508-4469-b52d-7c10b81a3c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf41526-1b26-45d0-838e-4c93fd435cf1",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression has a built-in feature that can help address multicollinearity in the input features to some extent. Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, which can lead to instability in the estimated coefficients.\n",
    "\n",
    "In the context of Lasso Regression, the L1 regularization term (absolute values of the coefficients) has the property of performing automatic feature selection by driving some coefficients to exactly zero. This means that Lasso has the ability to eliminate certain features from the model, effectively choosing a subset of features that contribute the most to predicting the target variable.\n",
    "\n",
    "The feature selection property of Lasso can help mitigate the impact of multicollinearity by \"choosing\" one of the correlated features and setting the coefficients of the others to zero. By doing so, Lasso implicitly selects a subset of features and can provide a more stable solution in the presence of multicollinearity.\n",
    "\n",
    "However, it's important to note that the effectiveness of Lasso in handling multicollinearity depends on the degree of correlation among the features. In some cases, Lasso may not entirely eliminate all correlated features, and the choice of the regularization strength parameter (alpha) plays a role in controlling the amount of shrinkage and feature selection.\n",
    "\n",
    "If multicollinearity is a significant concern, it might also be worth considering Ridge Regression, which uses L2 regularization and tends to shrink correlated coefficients towards each other without driving them to exactly zero. Ridge Regression can help in reducing the impact of multicollinearity while keeping all features in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626ee902-08f2-49db-be95-a0856a877302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20ffb9-9f1e-46f2-89c8-5aa0e4574491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a1aa14-2ade-4874-b3bf-a4ec11c7597a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025aa308-713b-4df9-83f3-d26decdc5a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e25a386-76f7-4521-b878-3a7572ebe5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
