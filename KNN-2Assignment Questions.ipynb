{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db9df8-78b9-4987-b038-c4a0e73af547",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Euclidean vs. Manhattan Distance Metric in KNN\n",
    "\n",
    "Euclidean Distance: This common metric calculates the straight-line distance between two data points in n-dimensional space. \n",
    "It considers all feature differences equally.\n",
    "\n",
    "Manhattan Distance: This metric represents the total distance traveled along each axis to get from one point to another. \n",
    "It emphasizes differences in specific features.\n",
    "\n",
    "Performance Impact:\n",
    "\n",
    "Euclidean Distance: More sensitive to outliers due to potentially large contributions from single feature differences. \n",
    "Performs well when features are independent and scaled similarly.\n",
    "Manhattan Distance: Less sensitive to outliers but may be less effective when features have varying scales or are highly correlated.\n",
    "Choosing the Distance Metric:\n",
    "\n",
    "Consider feature independence and scaling.\n",
    "Experiment with both metrics on your specific data to see which yields better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ed4a5-a85f-44e4-aa95-0e726995bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Choosing the Optimal Value of k\n",
    "\n",
    "The optimal value of k (number of nearest neighbors) significantly impacts KNN performance. Here's how to find it:\n",
    "\n",
    "Cross-validation: Split your data into training and validation sets.\n",
    "Train the KNN model with different k values on the training set and evaluate performance on the validation set. \n",
    "Choose the k that minimizes the error metric (e.g., classification error or mean squared error). \n",
    "Techniques like k-fold cross-validation are commonly used.\n",
    "Grid search or randomized search: These techniques systematically or randomly explore a range of k values, evaluating performance to find the optimal one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27315686-35f4-4e81-b2c6-540329ef4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Distance Metric and KNN Performance\n",
    "\n",
    "The distance metric influences how neighbors are identified:\n",
    "\n",
    "Euclidean Distance: May favor points concentrated in specific regions of the feature space.\n",
    "Manhattan Distance: Can be beneficial when dealing with grid-like data structures or when specific features are more important.\n",
    "Choosing a Distance Metric:\n",
    "\n",
    "Euclidean distance is often a good default, but experiment with Manhattan distance for specific data characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad0dfd4-3d22-488b-8c2c-7e3a9f0e4f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Common KNN Hyperparameters\n",
    "\n",
    "k: As discussed above, significantly impacts performance.\n",
    "Distance Metric: Choose between Euclidean, Manhattan, or others depending on your data.\n",
    "Normalization/Standardization: Scaling features often improves performance, especially when using Euclidean distance.\n",
    "Weighted Neighbors: Assign weights to neighbor points based on their distance to the query point, giving closer neighbors more influence.\n",
    "Hyperparameter Tuning:\n",
    "\n",
    "Grid search or randomized search can be used to explore different combinations of hyperparameters and find the best configuration.\n",
    "Feature engineering techniques like dimensionality reduction can improve performance by focusing on relevant features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e716c9-c2b0-4b92-ab22-abc083b84094",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Training Set Size and KNN\n",
    "\n",
    "Larger training sets: Generally lead to better performance as KNN relies on finding similar neighbors.\n",
    "\n",
    "Optimization Techniques:\n",
    "\n",
    "KD-Trees (K-Dimensional Trees): Efficient data structures to find nearest neighbors in high-dimensional data.\n",
    "Locality Sensitive Hashing (LSH): Technique for approximate nearest neighbor search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d226856-21f1-426e-a749-d2082802c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. Drawbacks of KNN and Improvements\n",
    "\n",
    "Computationally Expensive: Finding nearest neighbors can be slow for large datasets. KD-Trees and LSH can help.\n",
    "Curse of Dimensionality: Performance degrades in high-dimensional spaces. Feature selection or dimensionality reduction techniques can mitigate this.\n",
    "Sensitive to Noise and Outliers: Consider noise reduction techniques or using robust distance metrics that are less outlier-sensitive."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
